# -*- coding: utf-8 -*-
"""dino_detection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H5X29AuP2X8XBv2QFLFmdVskB9CMnH56
"""

# from google.colab import drive
# drive.mount('/content/drive')

# !python --version

# !nvidia-smi

import os

"""## Install Grounding DINO and Segment Anything Model

Our project will use two groundbreaking designs - [Grounding DINO](https://github.com/IDEA-Research/GroundingDINO) - for zero-shot detection and [Segment Anything Model (SAM)](https://github.com/facebookresearch/segment-anything) - for converting boxes into segmentations. We have to install them first.

"""

# # Commented out IPython magic to ensure Python compatibility.
# # %mkdir /content/drive/MyDrive/dino_detection
# # %cd /content/drive/MyDrive/dino_detection
# HOME = os.getcwd()
HOME = '/home/mustar/test_ws/src/detection/GroundingDINO/'
print("HOME:", HOME)

# # Commented out IPython magic to ensure Python compatibility.
# # %cd {HOME}
# !git clone https://github.com/IDEA-Research/GroundingDINO.git
# # %cd {HOME}/GroundingDINO
# !git checkout -q 57535c5a79791cb76e36fdb64975271354f10251
# !pip install -q -e .

"""### Download Grounding DINO Model Weights

To run Grounding DINO we need two files - configuration and model weights. The configuration file is part of the [Grounding DINO](https://github.com/IDEA-Research/GroundingDINO) repository, which we have already cloned. The weights file, on the other hand, we need to download. We write the paths to both files to the `GROUNDING_DINO_CONFIG_PATH` and `GROUNDING_DINO_CHECKPOINT_PATH` variables and verify if the paths are correct and the files exist on disk.
"""

GROUNDING_DINO_CONFIG_PATH = os.path.join(HOME, "groundingdino/config/GroundingDINO_SwinT_OGC.py")
print(GROUNDING_DINO_CONFIG_PATH, "; exist:", os.path.isfile(GROUNDING_DINO_CONFIG_PATH))

# # Commented out IPython magic to ensure Python compatibility.
# # %cd {HOME}
# !mkdir -p {HOME}/weights
# # %cd {HOME}/weights

# !wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth

# /home/mustar/test_ws/src/detection/GroundingDINO/weights/groundingdino_swint_ogc.pth
GROUNDING_DINO_CHECKPOINT_PATH = os.path.join(HOME, "weights", "groundingdino_swint_ogc.pth")
print(GROUNDING_DINO_CHECKPOINT_PATH, "; exist:", os.path.isfile(GROUNDING_DINO_CHECKPOINT_PATH))

"""## Load models"""

import torch

DEVICE = 'cpu'#torch.device('cuda' if torch.cuda.is_available() else 'cpu')

"""### Load Grounding DINO Model"""

# Commented out IPython magic to ensure Python compatibility.
# %cd {HOME}/GroundingDINO


from groundingdino.util.inference import Model

print("hello")
grounding_dino_model = Model(model_config_path=GROUNDING_DINO_CONFIG_PATH, model_checkpoint_path=GROUNDING_DINO_CHECKPOINT_PATH)

"""## Single Image Object Searching

Before we automatically annotate the entire dataset let's focus for a moment on a single image.

### Using **Prompt**:
"""

import cv2 as cv
# from google.colab.patches import cv2_imshow
import supervision as sv
import tensorflow as tf
from groundingdino.util.inference import load_model, load_image, predict, annotate

grounding_dino_model = load_model(GROUNDING_DINO_CONFIG_PATH, GROUNDING_DINO_CHECKPOINT_PATH)


SOURCE_IMAGE_PATH = "/home/mustar/test_ws/src/detection/GroundingDINO/raw_image.jpg"
TEXT_PROMPT = "grapes"
BOX_TRESHOLD = 0.4
TEXT_TRESHOLD = 0.25

image_source, image = load_image(SOURCE_IMAGE_PATH)
gray = cv.imread('/home/mustar/test_ws/src/detection/GroundingDINO/raw_image.jpg')
tensor = tf.convert_to_tensor(gray, dtype=tf.float32)
cv.imshow(gray)
# print(image)
# print(tensor)

boxes, logits, phrases = predict(
    model=grounding_dino_model,
    image=image,
    caption=TEXT_PROMPT,
    box_threshold=BOX_TRESHOLD,
    text_threshold=TEXT_TRESHOLD
)

annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)

# cv2_imshow(annotated_frame)
cv.imshow('image', annotated_frame)

# from PIL import Image
# import torch

# def load_image_as_tensor(frame_bgr):
#     """Load an image from disk using OpenCV and return it as a PyTorch tensor."""
#     # Load the image using OpenCV
#     # image_cv = cv2.imread(image_path)
#     # cv2_imshow(frame_bgr)

#     # Convert the image to RGB format
#     image_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)

#     # cv2_imshow(image_rgb)

#      # Convert the pixel values to the range [-1, 1] by subtracting the mean and dividing by the standard deviation
#     mean = torch.tensor([0.485, 0.456, 0.406])
#     std = torch.tensor([0.229, 0.224, 0.225])
#     image_tensor = torch.tensor(image_rgb / 255.0).permute(2, 0, 1)
#     image_tensor = (image_tensor - mean[:, None, None]) / std[:, None, None]

#     # Convert the data type to torch.float32 to match the original output
#     image_tensor = image_tensor.float()


#     return image_rgb, image_tensor
# # # Example usage:
# # image_path = "/content/drive/MyDrive/dino_detection/Images/owl.png"
# # image_tensor = load_image_as_tensor(image_path)
# # print(image_tensor)

# import cv2
# from google.colab.patches import cv2_imshow
# import supervision as sv
# from groundingdino.util.inference import load_model, load_image, predict, annotate

# grounding_dino_model = load_model(GROUNDING_DINO_CONFIG_PATH, GROUNDING_DINO_CHECKPOINT_PATH)

# def dino(frame, TEXT_PROMPT):

#   # SOURCE_IMAGE_PATH = "/content/drive/MyDrive/dino_detection/Images/fruits.jpg"
#   # TEXT_PROMPT = "can"
#   BOX_TRESHOLD = 0.2
#   TEXT_TRESHOLD = 0.25

#   # image_source, image = load_image(SOURCE_IMAGE_PATH)
#   image_source, tensor = load_image_as_tensor(frame)

#   boxes, logits, phrases = predict(
#       model=grounding_dino_model,
#       image=tensor,
#       caption=TEXT_PROMPT,
#       box_threshold=BOX_TRESHOLD,
#       text_threshold=TEXT_TRESHOLD
#   )

#   annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)

#   cv2_imshow(annotated_frame)


# # image_path = "/content/drive/MyDrive/dino_detection/Images/fruits.jpg"
# # image_cv = cv2.imread(image_path)
# # dino(image_cv)

# from google.colab.patches import cv2_imshow
# import numpy as np
# import cv2 as cv
# # gray = cv.imread('/content/drive/MyDrive/dino_detection/Images/fruits.jpg')
# # cv2_imshow(gray)
# cap = cv.VideoCapture('/content/drive/MyDrive/dino_detection/Videos/Cans.mp4')
# # Check if camera opened successfully
# if (cap.isOpened()== False):
#   print("Error opening video stream or file")

# # Read until video is completed
# while(cap.isOpened()):
#   # Capture frame-by-frame
#   ret, frame = cap.read()
#   if ret == True:

#     # Display the resulting frame
#     # cv2_imshow(frame)
#     dino(frame, 'yellow corn')
#     # break

#     # Press Q on keyboard to  exit
#     if cv2.waitKey(25) & 0xFF == ord('q'):
#       break

#   # Break the loop
#   else:
#     break

# # When everything done, release the video capture object
# cap.release()

# # Closes all the frames
# cv2.destroyAllWindows()

# # SOURCE_IMAGE_PATH = "/content/drive/MyDrive/dino_detection/Images/owl.png"
# # TEXT_PROMPT = "grapes"
# # BOX_TRESHOLD = 0.35
# # TEXT_TRESHOLD = 0.25
# # load image


# # # detect objects
# # detections = grounding_dino_model.predict_with_caption(
# #     image=image,
# #     caption=TEXT_PROMPT,
# #     box_threshold=BOX_TRESHOLD,
# #     text_threshold=TEXT_TRESHOLD
# # )

# # labels = []
# # print(detections[0])
# # for box in detections[0]:
# #   print(box[2])
#   # labels.append(f"{box[2]:0.2f}")
# # labels
# # box_annotator = sv.BoxAnnotator()
# # annotated_frame = box_annotator.annotate(scene=image.copy(), detections=detections[0],labels=labels)
# # sv.plot_image(annotated_frame, (16, 16))

# import cv2
# xyxy=[]
# # convert detections to masks
# for box in detections[0]:
#   xyxy.append(box[0])
# print(xyxy[0])

# import cv2
# import numpy as np
# import supervision as sv

# image_bgr = cv2.imread(SOURCE_IMAGE_PATH)
# image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)

# sam_predictor.set_image(image_rgb)

# masks, scores, logits = sam_predictor.predict(
#     box=xyxy[0],
#     multimask_output=True
# )

# box_annotator = sv.BoxAnnotator(color=sv.Color.red())
# mask_annotator = sv.MaskAnnotator(color=sv.Color.red())  # Remove color_lookup parameter

# detections = sv.Detections(
#     xyxy=sv.mask_to_xyxy(masks=masks),
#     mask=masks
# )
# detections = detections[detections.area == np.max(detections.area)]

# source_image = box_annotator.annotate(scene=image_bgr.copy(), detections=detections, skip_label=True)
# segmented_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)

# sv.plot_images_grid(
#     images=[source_image, segmented_image],
#     grid_size=(1, 2),
#     titles=['source image', 'segmented image']
# )

# """### Using **Classes**"""

# import numpy as np
# from segment_anything import SamPredictor


# def segment(sam_predictor: SamPredictor, image: np.ndarray, xyxy: np.ndarray) -> np.ndarray:
#     sam_predictor.set_image(image)
#     result_masks = []
#     for box in xyxy:
#         masks, scores, logits = sam_predictor.predict(
#             box=box,
#             multimask_output=True
#         )
#         index = np.argmax(scores)
#         result_masks.append(masks[index])
#     return np.array(result_masks)

# from typing import List

# def enhance_class_name(class_names: List[str]) -> List[str]:
#     return [
#         f"all {class_name}s"
#         for class_name
#         in class_names
#     ]

# SOURCE_IMAGE_PATH = "/content/drive/MyDrive/dino_detection/Images/fruits.jpg"
# CLASSES = ['apple', "grape" , "mango"]
# BOX_TRESHOLD = 0.35
# TEXT_TRESHOLD = 0.25

# import cv2
# import supervision as sv

# # load image
# image = cv2.imread(SOURCE_IMAGE_PATH)

# # detect objects
# detections = grounding_dino_model.predict_with_classes(
#     image=image,
#     classes=enhance_class_name(class_names=CLASSES),
#     box_threshold=BOX_TRESHOLD,
#     text_threshold=TEXT_TRESHOLD
# )
# detections
# # annotate image with detections
# box_annotator = sv.BoxAnnotator()
# print(detections)
# # labels = [
# #     f"{CLASSES[class_id]} {confidence:0.2f}"
# #     for _, _, confidence, class_id, _
# #     in detections]

# # annotated_frame = box_annotator.annotate(scene=image.copy(), detections=detections, labels=labels)

# # %matplotlib inline
# # sv.plot_image(annotated_frame, (16, 16))